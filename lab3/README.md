# LLM Prompt Injection Lab 3

This is a Flask application that demonstrates prompt injection vulnerabilities. But this time with [GuardRails AI](https://github.com/guardrails-ai/guardrails) in place!


## Requirements

- Python 3.x
- Flask
- openai
- guardrails-ai




