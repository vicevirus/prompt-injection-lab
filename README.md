# prompt-injection-lab

Welcome to prompt-injection-lab, a project focused on prompt injection lab.

## Introduction

The prompt-injection-lab project aims to provide a hands-on learning experience for prompt injection techniques. Prompt injection is a security vulnerability that occurs when an application fails to properly sanitize user input before using it in a command prompt or shell. This can lead to various security risks, such as command injection, remote code execution, or privilege escalation.

In this project, we are testing out few ways to secure prompt injection.

[Rebuff - LLM Prompt Injection Detector](https://github.com/protectai/rebuff)
[Guardrails AI - Adding guardrails to LLM](https://github.com/guardrails-ai/guardrails)


## Getting Started

To get started with prompt-injection-lab, please follow the instructions below:

1. Check out each lab folders README.md
2. Enjoy!

## Contributing

We welcome contributions from the community to enhance the prompt-injection-lab project. If you have any suggestions, bug reports, or would like to contribute code, please refer to our [Contribution Guidelines](CONTRIBUTING.md).

## License

prompt-injection-lab is released under the [MIT License](LICENSE).
