# prompt-injection-lab

Welcome to prompt-injection-lab, a project focused on prompt injection lab.

## Introduction

The prompt-injection-lab project aims to provide a hands-on learning experience for prompt injection techniques. Prompt injection is a security vulnerability that occurs when an application fails to properly sanitize user input before using it in a command prompt or shell. This can lead to various security risks, such as command injection, remote code execution, or privilege escalation.

In this project, we are exploring different techniques to enhance the security of a web application against prompt injection. Initially, the web app is vulnerable to Server-Side Template Injection (SSTI) in the response output.



## Labs

- Lab 1: Vulnerable LLM App
- Lab 2: Vulnerable LLM App with [Rebuff - LLM Prompt Injection Detector](https://github.com/protectai/rebuff)
- Lab 3: Vulnerable LLM App with [Guardrails AI - Adding guardrails to LLM](https://github.com/guardrails-ai/guardrails) (OpenAI)
- Lab 4: Vulnerable LLM App with [Guardrails AI - Adding guardrails to LLM](https://github.com/guardrails-ai/guardrails) (Google Gemini)



## Getting Started

To get started with prompt-injection-lab, please follow the instructions below:

1. Check out each lab folders README.md
2. Enjoy!

## Contributing

We welcome contributions from the community to enhance the prompt-injection-lab project. If you have any suggestions, bug reports, or would like to contribute code, please refer to our [Contribution Guidelines](CONTRIBUTING.md).

## License

prompt-injection-lab is released under the [MIT License](LICENSE).
