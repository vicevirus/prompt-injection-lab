# LLM Prompt Injection Lab 4

This is a Flask application that demonstrates prompt injection vulnerabilities. But this time with [GuardRails AI](https://github.com/guardrails-ai/guardrails) in place!

Gemini Pro ver.


## Requirements

- Python 3.x
- Flask
- openai
- guardrails-ai
- litellm
- google-cloud-aiplatform




